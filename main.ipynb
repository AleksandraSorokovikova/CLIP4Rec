{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:18:26.796212Z",
     "start_time": "2024-05-21T17:18:25.372427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.train import *\n",
    "from src.processing import *\n",
    "from src.models import *\n",
    "from src.inference import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:18:43.644736Z",
     "start_time": "2024-05-21T17:18:27.694082Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MPNetTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/CLIP4Rec/src/processing.py:20: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  movies_metadata = pd.read_csv(movies_metadata_path)\n",
      "100%|██████████| 865083/865083 [00:02<00:00, 297386.65it/s]\n",
      "100%|██████████| 7315/7315 [00:07<00:00, 927.15it/s]\n",
      "100%|██████████| 216271/216271 [00:00<00:00, 430673.83it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "tokenizer = BertTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "model = BertModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "ratings_df, movie_descriptions, movies_metadata = create_ratings_df(\n",
    "    number_of_movies=7500,\n",
    "    links_path='CLIP4Rec/archive/links.csv',\n",
    "    movies_metadata_path='CLIP4Rec/archive/movies_metadata.csv',\n",
    "    ratings_path='CLIP4Rec/archive/ratings.csv'\n",
    "    )\n",
    "sequences = get_sequences(ratings_df)\n",
    "vocab.build_vocab(sequences)\n",
    "\n",
    "train_sentences, val_sentences = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "train_data, film_descriptions_encoded = prepare_dataset(\n",
    "    train_sentences, movie_descriptions, tokenizer, vocab, encode_descriptions=True\n",
    ")\n",
    "val_data = prepare_dataset(\n",
    "    val_sentences, movie_descriptions, tokenizer, vocab\n",
    ")\n",
    "\n",
    "train_dataset = FilmRecommendationDataset(train_data, film_descriptions_encoded)\n",
    "val_dataset = FilmRecommendationDataset(val_data, film_descriptions_encoded)\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13517"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:19:43.661065Z",
     "start_time": "2024-05-21T17:19:43.585978Z"
    }
   },
   "outputs": [],
   "source": [
    "film_encoder = SASFilmEncoder(item_num=len(vocab.word_to_index), seq_len=seq_len, embed_dim=384, device=device)\n",
    "text_encoder = TextEncoder(model, output_dim=384, add_fc_layer=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 3000/13517 [14:49<51:56,  3.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 3000\n",
      "Accuracy: 0.0139\n",
      "Agreggated loss: 2.8955\n",
      "Classification loss: 7.0607\n",
      "Contrastive loss: 1.2480\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 6000/13517 [29:37<37:03,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 6000\n",
      "Accuracy: 0.0235\n",
      "Agreggated loss: 2.8535\n",
      "Classification loss: 6.8408\n",
      "Contrastive loss: 1.2159\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 9000/13517 [44:24<22:19,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 9000\n",
      "Accuracy: 0.0274\n",
      "Agreggated loss: 2.8311\n",
      "Classification loss: 6.7158\n",
      "Contrastive loss: 1.2021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 12000/13517 [59:12<07:29,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 12000\n",
      "Accuracy: 0.0288\n",
      "Agreggated loss: 2.8166\n",
      "Classification loss: 6.6332\n",
      "Contrastive loss: 1.1936\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13517/13517 [1:06:42<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 13517\n",
      "Accuracy: 0.0307\n",
      "Agreggated loss: 2.8108\n",
      "Classification loss: 6.6005\n",
      "Contrastive loss: 1.1904\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 337/3380 [01:12<10:52,  4.66it/s]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "train_clip(film_encoder, text_encoder, train_loader, val_loader, \n",
    "           epochs=epochs, lr=lr, device=device, iter_verbose=3000, folder='CLIP4Rec/artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(film_encoder.state_dict(), 'artifacts/film_encoder_weights_final.pth')\n",
    "# torch.save(text_encoder.state_dict(), 'artifacts/text_encoder_weights_final.pth')\n",
    "\n",
    "torch.save(train_dataset, 'CLIP4Rec/artifacts/train_dataset.pt')\n",
    "torch.save(val_dataset, 'CLIP4Rec/artifacts/val_dataset.pt')\n",
    "\n",
    "with open('CLIP4Rec/artifacts/ratings_df.pickle', 'wb') as f:\n",
    "  pickle.dump(ratings_df, f)\n",
    "\n",
    "with open('CLIP4Rec/artifacts/movie_descriptions.pickle', 'wb') as f:\n",
    "  pickle.dump(movie_descriptions, f)\n",
    "\n",
    "with open('CLIP4Rec/artifacts/sequences.pickle', 'wb') as f:\n",
    "  pickle.dump(sequences, f)\n",
    "\n",
    "with open('CLIP4Rec/artifacts/vocab.pickle', 'wb') as f:\n",
    "  pickle.dump(vocab, f)\n",
    "\n",
    "with open('CLIP4Rec/artifacts/film_descriptions_encoded.pickle', 'wb') as f:\n",
    "  pickle.dump(film_descriptions_encoded, f)\n",
    "\n",
    "with open('CLIP4Rec/artifacts/movies_metadata.pickle', 'wb') as f:\n",
    "  pickle.dump(movies_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:05:53.544682Z",
     "start_time": "2024-05-21T16:05:53.539389Z"
    }
   },
   "outputs": [],
   "source": [
    "# list_movies = [\"Only Lovers Left Alive\",\n",
    "#                \"The Twilight Saga: Eclipse\",\n",
    "#                \"Me Before You\",\n",
    "#                \"(500) Days of Summer\"]\n",
    "\n",
    "list_movies = [\"Minions\",\n",
    "               \"Zootopia\",\n",
    "               \"Shrek\",\n",
    "               \"Kung Fu Panda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:05:56.673169Z",
     "start_time": "2024-05-21T16:05:53.544180Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Aleksandra.Sorokovikova/Study/Data Science/mamba rec system/CLIP4Rec/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26073a460064c339d37bb86d3dbb3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = pd.read_pickle('artifacts/vocab.pickle')\n",
    "movies_metadata = pd.read_pickle('artifacts/movies_metadata.pickle')\n",
    "film_descriptions_encoded = pd.read_pickle('artifacts/film_descriptions_encoded.pickle')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "dim = 384\n",
    "num_trees=10\n",
    "search_type='euclidean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:08<00:00, 28.22it/s]\n",
      "100%|██████████| 7314/7314 [00:01<00:00, 7024.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# build and save\n",
    "\n",
    "inference = Inference(\n",
    "    film_encoder_path = 'artifacts/film_encoder_weights_final_4.pth',\n",
    "    text_encoder_path = 'artifacts/text_encoder_weights_final_4.pth',\n",
    "    vocab=vocab,\n",
    "    dim=dim,\n",
    "    movies_metadata=movies_metadata,\n",
    "    seq_len=seq_len,\n",
    "    device=device,\n",
    "    bert_model=bert_model,\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    ")\n",
    "\n",
    "film_embeddings, text_embeddings = inference.get_embeddings(film_descriptions_encoded, batch_size=32)\n",
    "\n",
    "annoy_model = AnnoySearchEngine(\n",
    "    dim=dim,\n",
    "    num_trees=num_trees,\n",
    "    search_type=search_type,\n",
    ")\n",
    "annoy_model.build_trees(film_embeddings, text_embeddings)\n",
    "annoy_model.save_indexes('artifacts/text_index.ann', 'artifacts/film_index.ann', 'artifacts/idx_to_movieId.pickle')\n",
    "inference.init_annoy_model('artifacts/text_index.ann', 'artifacts/film_index.ann', 'artifacts/idx_to_movieId.pickle', num_trees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kung Fu Panda',\n",
       " 'Kung Fu Panda 2',\n",
       " 'The Mermaid',\n",
       " 'Kung Fu Dunk',\n",
       " 'The Man with the Iron Fists 2',\n",
       " 'Girls Against Boys',\n",
       " 'Shanghai Knights',\n",
       " 'Clean',\n",
       " 'Rise: Blood Hunter',\n",
       " 'Saving Mr. Wu']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview = movies_metadata.query('title==\"Kung Fu Panda\"')['overview'].values[0]\n",
    "inference.search_text(overview, in_films=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:21:08.065808Z",
     "start_time": "2024-05-21T17:21:08.057161Z"
    }
   },
   "outputs": [],
   "source": [
    "# load and init\n",
    "\n",
    "inference = Inference(\n",
    "    film_encoder_path = 'artifacts/film_encoder_weights_final_4.pth',\n",
    "    text_encoder_path = 'artifacts/text_encoder_weights_final_4.pth',\n",
    "    vocab=vocab,\n",
    "    dim=384,\n",
    "    movies_metadata=movies_metadata,\n",
    "    seq_len=seq_len,\n",
    "    device=device,\n",
    "    bert_model=bert_model,\n",
    "    bert_tokenizer=bert_tokenizer,\n",
    ")\n",
    "\n",
    "inference.init_annoy_model('artifacts/text_index.ann', 'artifacts/film_index.ann', 'artifacts/idx_to_movieId.pickle', num_trees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Megamind',\n",
       " 'Despicable Me 2',\n",
       " 'Teenage Mutant Ninja Turtles: Out of the Shadows',\n",
       " 'The Lego Movie',\n",
       " 'Teen Titans: Trouble in Tokyo',\n",
       " 'LEGO DC Comics Super Heroes: Justice League: Attack of the Legion of Doom!',\n",
       " 'Superman/Shazam!: The Return of Black Adam',\n",
       " 'The SpongeBob SquarePants Movie',\n",
       " 'In the Name of the King 2: Two Worlds',\n",
       " 'Fantastic 4: Rise of the Silver Surfer']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview = movies_metadata.query('title==\"Megamind\"')['overview'].values[0]\n",
    "inference.search_text(overview, in_films=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
